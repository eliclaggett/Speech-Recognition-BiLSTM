{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "File Name: main.ipynb\n",
        "Author: Eli Claggett\n",
        "Date: Apr 2023\n",
        "\n",
        "Description:\n",
        "    In-class project implementing a bidirectional long short-term memory (BiLSTM) neural network for speech recognition\n",
        "    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ZTCIXoof2f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "# from torchsummaryX import summary\n",
        "from torch.autograd import Variable\n",
        "import torchaudio.transforms as tat\n",
        "from collections import namedtuple\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "import ctcdecode\n",
        "import datetime\n",
        "import warnings\n",
        "import zipfile\n",
        "import random\n",
        "import torch\n",
        "import wandb\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# ARPABET phoneme mapping\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]\n",
        "p2iMap = {p:i for i, p in enumerate(PHONEMES)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "# Create audio dataset\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition, phoneme_dict = p2iMap): \n",
        "        self.phoneme_dict = phoneme_dict\n",
        "\n",
        "        self.mfcc_dir       =  partition+\"/mfcc/\"         \n",
        "        self.transcript_dir = partition+\"/transcript/\" \n",
        "\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir)) \n",
        "        self.mfcc_names = [self.mfcc_dir + i for i in mfcc_names]\n",
        "        \n",
        "        transcript_names    = sorted(os.listdir(self.transcript_dir)) \n",
        "        self.transcript_names = [self.transcript_dir + i for i in transcript_names]\n",
        "        \n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "        self.length = len(mfcc_names)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = np.load(self.mfcc_names[ind])\n",
        "        \n",
        "        raw_phoneme = np.load(self.transcript_names[ind])\n",
        "        \n",
        "        # Get phonemes without SOS and EOS tokens\n",
        "        phoneme = np.array([self.phoneme_dict[i] for i in raw_phoneme[1:-1]])\n",
        "\n",
        "        return torch.tensor(mfcc), torch.tensor(phoneme)\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        1.  Extract the features and labels from each batch \n",
        "        2.  Pad both features and labels\n",
        "        3.  Perform transforms on batches (if desired)\n",
        "        4.  Return batch of features, labels, feature lengths, and label lengths.\n",
        "        '''\n",
        "\n",
        "        batch_mfcc = [x for x, y in batch]\n",
        "        batch_transcript = [y for x, y in batch]\n",
        "        \n",
        "        norm_mfcc = []\n",
        "        for mfcc in batch_mfcc:\n",
        "            mfcc -= torch.mean(mfcc, axis=0, keepdims=True)\n",
        "            mfcc /= torch.std(mfcc, axis=0, keepdims=True)\n",
        "            norm_mfcc.append(mfcc)\n",
        "        \n",
        "        batch_mfcc_pad = pad_sequence(norm_mfcc, batch_first=True)\n",
        "        lengths_mfcc = [i.shape[0] for i in norm_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "        lengths_transcript = [i.shape[0] for i in batch_transcript]\n",
        "\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Create audio dataset for test data\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition, phoneme_dict = p2iMap): \n",
        "        self.phoneme_dict = phoneme_dict\n",
        "        \n",
        "        self.mfcc_dir       =  partition+\"/mfcc/\" \n",
        "        \n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir)) \n",
        "        self.mfcc_names = [self.mfcc_dir + i for i in mfcc_names]\n",
        "        \n",
        "        self.length = len(mfcc_names)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = np.load(self.mfcc_names[ind])\n",
        "        return torch.tensor(mfcc)\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        1.  Extract the features and labels from each batch \n",
        "        2.  Pad both features and labels\n",
        "        3.  Perform transforms on batches (if desired)\n",
        "        4.  Return batch of features, labels, feature lengths, and label lengths.\n",
        "        '''\n",
        "        \n",
        "        batch_mfcc = [x for x in batch]\n",
        "        \n",
        "        # Do cepestral norm\n",
        "        norm_mfcc = []\n",
        "        for mfcc in batch_mfcc:\n",
        "            mfcc = mfcc - torch.mean(mfcc, axis=0, keepdims=True)\n",
        "            mfcc = mfcc / torch.std(mfcc, axis=0, keepdims=True)\n",
        "            norm_mfcc.append(mfcc)\n",
        "        \n",
        "        batch_mfcc_pad = pad_sequence(norm_mfcc, batch_first=True)\n",
        "        lengths_mfcc = [i.shape[0] for i in norm_mfcc]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_kG0gU2x4hH"
      },
      "outputs": [],
      "source": [
        "# Let the computer rest\n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 32,\n",
        "    'feature_dim': 27,\n",
        "    'rnn_hidden_size': 256,\n",
        "    'rnn_num_layers': 4,\n",
        "    'rnn_dropout': 0.35,\n",
        "    'rnn_residual': True,\n",
        "    'learning_rate': 1e-3,\n",
        "    'epochs' : 5,\n",
        "    'beam_width': 16,\n",
        "    'clip_thresh': 1.,\n",
        "    'context': 20,\n",
        "    'architecture': 'apc'\n",
        "}\n",
        "\n",
        "root = './data'\n",
        "\n",
        "# Set of transforms to apply to the dataset\n",
        "transforms = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Populate datasets\n",
        "train_data = AudioDataset(partition='./data/dev-clean')\n",
        "val_data = AudioDataset(partition='./data/dev-clean')\n",
        "test_data = AudioDatasetTest(partition='./data/test-clean')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 0,\n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 0,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn  = val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 0, \n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True, \n",
        "    shuffle     = False,\n",
        "    collate_fn  = test_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the bidirectional LSTM network\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size      = input_size,\n",
        "                            num_layers      = 4,\n",
        "                            hidden_size     = 256, \n",
        "                            dropout         = 0.4,\n",
        "                            batch_first     = True, \n",
        "                            bidirectional   = True) \n",
        "        self.classification = nn.Sequential(\n",
        "            torch.nn.Linear(512, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        input_pack = pack_padded_sequence(x, lx, \n",
        "                                          batch_first   = True,\n",
        "                                          enforce_sorted= False)\n",
        "        \n",
        "        out, _ = self.lstm(input_pack)\n",
        "        out, lens = pad_packed_sequence(out, batch_first=True)\n",
        "        \n",
        "        out = self.classification(out)\n",
        "        out = F.log_softmax(out, dim=2)\n",
        "        \n",
        "        return out, lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup the model\n",
        "RNNConfig = namedtuple(\n",
        "  'RNNConfig',\n",
        "  ['input_size', 'hidden_size', 'num_layers', 'dropout', 'residual'])\n",
        "\n",
        "# As described in the paper\n",
        "prenet_config = None \n",
        "rnn_config = RNNConfig(\n",
        "      config['feature_dim'],)\n",
        "\n",
        "model = Network(config['feature_dim'], 41).cuda()\n",
        "\n",
        "# Setup the training paradigm (scheduler, optimizer, loss function)\n",
        "criterion = nn.CTCLoss(zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'], 1e-5) \n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "decoder = CTCBeamDecoder(PHONEMES, log_probs_input=True, beam_width = config['beam_width'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "# Decode output of the model\n",
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    results, _, _, lens = decoder.decode(output, seq_lens= output_lens)\n",
        "\n",
        "    pred_strings = []\n",
        "    \n",
        "    for i in range(output_lens.shape[0]):\n",
        "        beam = results[i][0][:lens[i][0]]\n",
        "        mapped = ''.join([LABELS[i] for i in beam])\n",
        "        pred_strings.append(mapped)\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "# Calculate levenshtein distance as model performance metric\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        \n",
        "        pred_string = pred_strings[i]\n",
        "        \n",
        "        slices = label[i][:label_lens[i]]\n",
        "        \n",
        "        label_string = ''.join([LABELS[i] for i in slices])\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size \n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s52yBOvICPZ"
      },
      "outputs": [],
      "source": [
        "# Track training progress on Weights & Biases\n",
        "wandb.login(key='REDACTED')\n",
        "\n",
        "run = wandb.init(\n",
        "    name = config['architecture'],\n",
        "    reinit = True,\n",
        "    project = \"bilstm-ablations\",\n",
        "    config = config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "# Setup a training function\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast(): \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Prevent gradient vanishing with FP16 calculations\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_( model.parameters(),\n",
        "                                                    config.clip_thresh)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            grad_norm=\"{:.04f}\".format(float(grad_norm)))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Create a validation function\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "# Create a function to save the model state\n",
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "# Create a function to load a saved model\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "outputs": [],
      "source": [
        "# Let the computer rest\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Variables for tracking progress\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\")\n",
        "epoch_model_path = 'epoch.pth'\n",
        "best_model_path = 'best.bth'\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer)\n",
        "    valid_loss, valid_dist = validate_model(model, val_loader, decoder, phoneme_map= LABELS)\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    \n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      \n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "TEST_BEAM_WIDTH = config['beam_width'] * 2\n",
        "\n",
        "test_decoder = CTCBeamDecoder(PHONEMES, log_probs_input=True, beam_width = TEST_BEAM_WIDTH)\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string= decode_prediction(h, lh, decoder, PHONEME_MAP=LABELS)\n",
        "    results.append(prediction_string)\n",
        "    \n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "# Save test results\n",
        "template = './data/test-clean/transcript/result_template.csv'\n",
        "df = pd.read_csv(template)\n",
        "df.label = results\n",
        "df.to_csv('test_bilstm.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UR4qfYrVoO4v",
        "rd5aNaLVoR_g",
        "ONgAWhqdoYy-",
        "IWVONJxCobPc",
        "gg3-yJ8tok34",
        "R9v5ewZDMpYA",
        "Ly4mjUUUuJhy",
        "HLad4pChcuvX",
        "tUThsowyQdN7",
        "IBwunYpyugFg",
        "kH0RAbCaMl9a",
        "qpYExu4vT4_g",
        "MY69hgxUXhTI",
        "M2H4EEj-sD32"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
